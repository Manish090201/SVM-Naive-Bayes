{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is a Support Vector Machine (SVM), and how does it work?"
      ],
      "metadata": {
        "id": "AtRk924sm00s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-\tSVM is a discriminative classifier that works by finding the optimal hyperplane that separates data points of different classes with the maximum margin (i.e., the largest possible distance between data points of both classes).\n",
        "1. Linear SVM (for linearly separable data):\n",
        "‚Ä¢\tIn 2D space, a hyperplane is a straight line.\n",
        "‚Ä¢\tSVM finds the best line (hyperplane) that separates the two classes.\n",
        "‚Ä¢\tThe goal is to maximize the margin, which is the distance between the hyperplane and the closest data points from each class (called support vectors).\n",
        "Example:\n",
        "Class 1 ‚óè ‚óè ‚óè\n",
        "           |\n",
        "--------   ‚Üê Optimal Hyperplane\n",
        "           |\n",
        "Class 2 ‚óã ‚óã ‚óã\n",
        "\n",
        "2. Support Vectors:\n",
        "‚Ä¢\tThese are the critical data points that lie closest to the hyperplane.\n",
        "‚Ä¢\tThey influence the position and orientation of the hyperplane.\n",
        "‚Ä¢\tRemoving a support vector would change the model.\n",
        "\n",
        "3. Non-linear SVM:\n",
        "‚Ä¢\tFor data that isn't linearly separable, SVM uses a technique called the kernel trick.\n",
        "Kernel Trick:\n",
        "‚Ä¢\tMaps data from a low-dimensional space to a higher-dimensional space where it becomes linearly separable.\n",
        "‚Ä¢\tCommon kernels:\n",
        "o\tLinear Kernel\n",
        "o\tPolynomial Kernel\n",
        "o\tRBF (Radial Basis Function) or Gaussian Kernel\n",
        "o\tSigmoid Kernel\n",
        "\n"
      ],
      "metadata": {
        "id": "Zd4-w_7Em2Sc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM."
      ],
      "metadata": {
        "id": "CKchGDh7nI5w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Difference Between Hard Margin and Soft Margin SVM\n",
        "Support Vector Machines (SVM) aim to find the optimal hyperplane that separates data points of different classes. However, depending on the nature of the data (whether it's perfectly separable or has noise/overlap), two approaches are used:\n",
        "\n",
        "1. Hard Margin SVM\n",
        "Definition:\n",
        "Hard Margin SVM tries to find a hyperplane that perfectly separates the data without any misclassifications.\n",
        "Conditions:\n",
        "‚Ä¢\tWorks only when data is linearly separable.\n",
        "‚Ä¢\tNo data points are allowed inside the margin or on the wrong side of the hyperplane.\n",
        "Objective:\n",
        "‚Ä¢\tMaximize the margin\n",
        "‚Ä¢\tNo tolerance for misclassification.\n",
        "Disadvantages:\n",
        "‚Ä¢\tVery sensitive to noise or outliers.\n",
        "‚Ä¢\tNot suitable for real-world datasets that are rarely perfectly separable.\n",
        "2. Soft Margin SVM\n",
        "Definition:\n",
        "Soft Margin SVM allows some misclassification or margin violations to improve generalization on noisy or overlapping data.\n",
        "Conditions:\n",
        "‚Ä¢\tWorks for non-linearly separable data.\n",
        "‚Ä¢\tAllows trade-off between maximizing margin and minimizing classification error.\n",
        "Objective:\n",
        "‚Ä¢\tIntroduces a regularization parameter (C) to balance:\n",
        "o\tLarge margin (simplicity) and\n",
        "o\tSmall error (accuracy)\n",
        "C parameter:\n",
        "‚Ä¢\tSmall C: More tolerance for misclassification (larger margin).\n",
        "‚Ä¢\tLarge C: Less tolerance for error (narrow margin, stricter separation).\n"
      ],
      "metadata": {
        "id": "ErZA2nNgnMsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case."
      ],
      "metadata": {
        "id": "ZwylgJIRnR07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kernel Trick is a technique used in Support Vector Machines (SVM) to handle non-linearly separable data by implicitly mapping the original features into a higher-dimensional space ‚Äî without ever computing that mapping explicitly.\n",
        "\n",
        "This allows the SVM to find a linear hyperplane in the transformed space, which corresponds to a non-linear decision boundary in the original space.\n",
        "\n",
        "This is useful\n",
        "In many real-world datasets, the data cannot be separated by a straight line. The kernel trick helps SVM create non-linear decision boundaries using linear methods in higher dimensions.\n",
        "\n",
        "Without Kernel Trick:\n",
        "You would need to manually transform data using feature engineering.\n",
        "\n",
        "This could be computationally expensive and complex.\n",
        "\n",
        "With Kernel Trick:\n",
        "You just use a kernel function like K(x, y) instead of computing the transformed feature space.\n",
        "\n",
        "The function computes the dot product in the higher-dimensional space implicitly."
      ],
      "metadata": {
        "id": "i3AIoNlinUP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?"
      ],
      "metadata": {
        "id": "CSzsE-86nudX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Na√Øve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes‚Äô Theorem, used mainly for classification tasks.\n",
        "It predicts the class of a data point by calculating the probability of each class given the feature values and selecting the one with the highest probability.\n",
        "üìò Bayes' Theorem:\n",
        "Bayes' Theorem:\n",
        "P(C | X) = [P(X | C) * P(C)] / P(X)\n",
        "Where:\n",
        "P(C | X): Posterior probability of class C given features X  \n",
        "P(X | C): Likelihood of features X given class C  \n",
        "P(C): Prior probability of class C  \n",
        "P(X): Evidence (normalizing constant)\n",
        "It is called ‚Äúna√Øve‚Äù because it makes a na√Øve assumption.\n"
      ],
      "metadata": {
        "id": "q_xsJ2a0nzD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one?"
      ],
      "metadata": {
        "id": "kvPGkscrn3QG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Gaussian Na√Øve Bayes\n",
        "Description:\n",
        "Assumes that the features are continuous and follow a normal (Gaussian) distribution.\n",
        "Formula:\n",
        "Each feature is modeled using:\n",
        "$$\n",
        "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\mu)^2}{2\\sigma^2} \\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\tŒº: mean of the feature for class y,\n",
        "\tsigma^2: variance of the feature for class y\n",
        " Use Case:\n",
        "\tWhen input features are real-valued (continuous).\n",
        "\tE.g., Iris classification, medical data, sensor data.\n",
        "2. Multinomial Na√Øve Bayes\n",
        " Description:\n",
        "Assumes features represent discrete counts (e.g., number of times a word appears in a document). Often used in text classification problems.\n",
        " Formula:\n",
        "Uses the multinomial distribution to model the likelihood of a set of features given a class.\n",
        "$$\n",
        "P(X \\mid y) = \\frac{(\\sum x_i)!}{x_1! \\cdot x_2! \\cdots x_n!} \\prod_{i=1}^{n} P(x_i \\mid y)^{x_i}\n",
        "$$\n",
        "\n",
        "Use Case:\n",
        "\tWhen features are word counts, term frequencies, or frequency features.\n",
        "\tE.g., Spam detection, news classification, sentiment analysis.\n",
        "3. Bernoulli Na√Øve Bayes\n",
        "Description:\n",
        "Assumes features are binary (0 or 1), indicating presence or absence of a feature.\n",
        "Example:\n",
        "In text, instead of using word counts, Bernoulli NB uses whether a word exists in the document or not.\n",
        "Use Case:\n",
        "\tWhen features are binary.\n",
        "\tE.g., Short text classification, binary feature datasets, event occurrence prediction.\n"
      ],
      "metadata": {
        "id": "htdsGiT4n7p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6:   Write a Python program to:\n",
        "‚óè Load the Iris dataset\n",
        "‚óè Train an SVM Classifier with a linear kernel\n",
        "‚óè Print the model's accuracy and support vectors."
      ],
      "metadata": {
        "id": "fVpNksqVoyaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data       # Features\n",
        "y = iris.target     # Labels\n",
        "\n",
        "# Step 2: Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train an SVM classifier with a linear kernel\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Support Vectors:\\n\", model.support_vectors_)\n",
        "print(\"Support Vector Indices:\", model.support_)\n",
        "print(\"Number of Support Vectors for each class:\", model.n_support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqPdTSM8pD9M",
        "outputId": "5b211cc0-d29f-4011-a654-c54019a7dafc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "Support Vector Indices: [ 31  33  91  22  45  54  59  60  62  73  79  80 105 110   5  16  30  42\n",
            "  68  81  87 101 112 113 116]\n",
            "Number of Support Vectors for each class: [ 3 11 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Write a Python program to: ‚óè Load the Breast Cancer dataset ‚óè Train a Gaussian Na√Øve Bayes model ‚óè Print its classification report including precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "xVTXDhLbpIqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 2: Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Create and train the Gaussian Na√Øve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEl4-DPmpQzf",
        "outputId": "c0b53669-a3a8-4744-d71d-f831702d1123"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to: ‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. ‚óè Print the best hyperparameters and accuracy."
      ],
      "metadata": {
        "id": "aBF6VVFHpZxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Step 3: Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Define the SVM model and parameter grid\n",
        "model = SVC()\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']  # using RBF kernel\n",
        "}\n",
        "\n",
        "# Step 5: Perform Grid Search with cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions with the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Step 7: Print the results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NXvT3a3pVIj",
        "outputId": "ae4c8639-cabf-4c6e-d53f-af163f670bc4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to: ‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups). ‚óè Print the model's ROC-AUC score for its predictions."
      ],
      "metadata": {
        "id": "1jNUHDb5pfmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Step 2: Load a subset of the 20 Newsgroups dataset (binary classification for ROC-AUC)\n",
        "categories = ['sci.space', 'comp.graphics']  # binary classification\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target  # 0 or 1\n",
        "\n",
        "# Step 3: Convert text data to TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Step 4: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Train the Na√Øve Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict probabilities\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "\n",
        "# Step 7: Calculate and print ROC-AUC score\n",
        "auc = roc_auc_score(y_test, y_probs)\n",
        "print(\"ROC-AUC Score:\", auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tzKAfQcpj3w",
        "outputId": "f3abfcdb-b459-4ec0-fd71-dfcee0b3900e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9853776041666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you‚Äôre working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain: ‚óè Text with diverse vocabulary ‚óè Potential class imbalance (far more legitimate emails than spam) ‚óè Some incomplete or missing data Explain the approach you would take to: ‚óè Preprocess the data (e.g. text vectorization, handling missing data) ‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes) ‚óè Address class imbalance ‚óè Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution."
      ],
      "metadata": {
        "id": "kAshJd7wpwdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Problem Statement\n",
        "\n",
        "You are a data scientist tasked with building a classifier to label emails as **Spam** or **Not Spam**. The dataset contains:\n",
        "\n",
        "- Text with diverse vocabulary  \n",
        "- Class imbalance (more legitimate emails than spam)  \n",
        "- Incomplete or missing data in some records  \n",
        "\n",
        "---\n",
        "\n",
        "## 1.  Data Preprocessing\n",
        "\n",
        "### a. Handling Missing Data\n",
        "\n",
        "- Drop emails where both **subject** and **body** are missing.\n",
        "- Fill missing values with empty strings for individual fields.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Simulated loading\n",
        "df = pd.read_csv(\"emails.csv\")\n",
        "\n",
        "# Fill missing text fields\n",
        "df['subject'] = df['subject'].fillna(\"\")\n",
        "df['body'] = df['body'].fillna(\"\")\n",
        "\n",
        "# Combine subject and body into one feature\n",
        "df['text'] = df['subject'] + \" \" + df['body']\n"
      ],
      "metadata": {
        "id": "FOYLUyqhqS2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. Text Cleaning and Vectorization"
      ],
      "metadata": {
        "id": "3j6qZiB5qbV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(df['text'])\n",
        "\n",
        "# Target variable\n",
        "y = df['label']  # 1 for spam, 0 for not spam\n"
      ],
      "metadata": {
        "id": "TqqUaPrOqr__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Model Choice: SVM vs Na√Øve Bayes"
      ],
      "metadata": {
        "id": "pqvxLmBErFiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model       | Pros                                  | Cons                 |\n",
        "| ----------- | ------------------------------------- | -------------------- |\n",
        "| Na√Øve Bayes | Fast, good for text, interpretable    | Assumes independence |\n",
        "| SVM         | High accuracy, handles imbalance well | Slower, needs tuning |\n"
      ],
      "metadata": {
        "id": "I7QMvVQvq5M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Handling Class Imbalance\n"
      ],
      "metadata": {
        "id": "KqPBfGhhq9aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stratified split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "atddtCcgrLwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluation Metrics"
      ],
      "metadata": {
        "id": "kajLh4G7qykD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Na√Øve Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "y_prob_nb = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Naive Bayes Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "print(\"Naive Bayes ROC-AUC:\", roc_auc_score(y_test, y_prob_nb))\n"
      ],
      "metadata": {
        "id": "u9MysDNJrPli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM with probability calibration\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "svm = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_calibrated = CalibratedClassifierCV(svm)\n",
        "svm_calibrated.fit(X_train, y_train)\n",
        "y_pred_svm = svm_calibrated.predict(X_test)\n",
        "y_prob_svm = svm_calibrated.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "print(\"SVM ROC-AUC:\", roc_auc_score(y_test, y_prob_svm))\n"
      ],
      "metadata": {
        "id": "3m2CeedyrRyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Business Impact"
      ],
      "metadata": {
        "id": "4yfCYBQcqoNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Business Benefit | Explanation                                                |\n",
        "| ---------------- | ---------------------------------------------------------- |\n",
        "| ‚úÖ User Trust     | Prevents false spam blocks, improves satisfaction          |\n",
        "| ‚úÖ Security       | Blocks phishing, scams, and malicious links                |\n",
        "| ‚úÖ Efficiency     | Automates email filtering, reduces human moderation effort |\n",
        "| ‚úÖ Compliance     | Helps meet data protection and anti-spam laws              |\n"
      ],
      "metadata": {
        "id": "vSJmcDEIrW5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RbT3JY-wrZOL"
      }
    }
  ]
}